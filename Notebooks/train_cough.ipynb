{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import concurrent.futures\n",
    "# import librosa\n",
    "# import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "# import matplotlib.pyplot as plt\n",
    "# # import tensorflow as tf\n",
    "# # import pickle\n",
    "# # import json\n",
    "# import librosa.display\n",
    "# # from scipy import signal\n",
    "from statistics import mode\n",
    "# from scipy.io import wavfile\n",
    "import os\n",
    "# import skimage.io\n",
    "from tqdm import tqdm\n",
    "# from tensorflow.keras.applications.densenet import DenseNet121, DenseNet169\n",
    "# from tensorflow.keras.applications.vgg16 import VGG16\n",
    "# from tensorflow.keras.applications.resnet import ResNet101 \n",
    "# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "# import pickle\n",
    "import random\n",
    "from random import shuffle\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.metrics.pairwise import manhattan_distances, cosine_distances\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../pickle_files/cough_sound/cs_vgg16.pickle','rb') as handle:\n",
    "   dataset = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=dataset[:1312]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "shuffle(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_mispredictions(query, fea_label,train_label, train_id, ind_data, decision,data_frame_1, count):\n",
    "    if mode(decision) != query[\"label\"]:\n",
    "        count +=1 \n",
    "        data_frame_1[\"Image name\"].append(query[\"filepath\"].split(\"/\")[-1])\n",
    "        data_frame_1[\"Mistake ID\"].append(query['id'])\n",
    "        data_frame_1[\"Original label\"].append(query['label'])\n",
    "        data_frame_1[\"Predicted label\"].append(mode(decision))\n",
    "        data_frame_1[\"Mistake index\"].append(ind_data)\n",
    "        fea_label[query['label']].append(query[\"feature\"])\n",
    "        train_label[query['label']].append(query[\"label\"])\n",
    "        train_id[query['label']].append(query['id'])\n",
    "    else:\n",
    "        fea_label[query['label']].append(query[\"feature\"])\n",
    "        train_label[query['label']].append(query[\"label\"])\n",
    "        train_id[query['label']].append(query['id'])\n",
    "    return count,data_frame_1,fea_label,train_label,train_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance1(query, fea_label, select_distance, id_pred, label_pred, n_neighbours,count,train_label, train_id, ind_data, data_frame_1,supervised_data): # Query is the raw dictionary (from pickle file) // fea_label is dictionary of {0: [], 1:[]} (distance) // select distance is int\n",
    "  exp_query = np.expand_dims(query['feature'], axis=0)\n",
    "  pos_tup, neg_tup = [], []\n",
    "  \n",
    "  if select_distance==1: # Euclidean distance\n",
    "    \n",
    "    neg_dist = np.linalg.norm(query['feature']-fea_label[0], axis=1)  # Calculating the Euclidean distance using numpy (axis=1) to calculate all at ones   \n",
    "    pos_dist = np.linalg.norm(query['feature']-fea_label[1],axis=1)\n",
    "  \n",
    "  elif select_distance==2: # Manhattan distance\n",
    "     neg_dist = np.squeeze(manhattan_distances(fea_label[0],exp_query))  # convert (1,n) to (,n)\n",
    "     pos_dist=np.squeeze(manhattan_distances(fea_label[1],exp_query))\n",
    "\n",
    "  elif select_distance==3: # Cosine distance\n",
    "    neg_dist = np.squeeze(cosine_distances(exp_query,fea_label[0]))  # convert (1,n) to (,n)\n",
    "    pos_dist=np.squeeze(cosine_distances(exp_query,fea_label[1]))\n",
    "\n",
    "  \n",
    "  for dist_single in pos_dist:\n",
    "    pos_tup.append((dist_single,1))\n",
    "\n",
    "  for dist_single in neg_dist:\n",
    "    neg_tup.append((dist_single,0))\n",
    "\n",
    "  pos_tup.extend(neg_tup)\n",
    "  tup_dist = sorted(pos_tup)[:n_neighbours]\n",
    "  \n",
    "  \n",
    "  decision = [y for (x,y) in tup_dist]\n",
    "  if supervised_data:\n",
    "    count,data_frame_1,fea_label,train_label,train_id=correct_mispredictions(query, fea_label,train_label,train_id, ind_data, decision,data_frame_1, count)\n",
    "  \n",
    "  else:\n",
    "    if decision.count(0) > decision.count(1):\n",
    "      fea_label[0].append(query[\"feature\"])\n",
    "      id_pred[0].append(query[\"id\"])\n",
    "      label_pred[0].append((query['id'],decision.count(1)/n_neighbours))\n",
    "\n",
    "    else:\n",
    "      fea_label[1].append(query[\"feature\"])\n",
    "      id_pred[1].append(query[\"id\"])\n",
    "      label_pred[1].append((query['id'],decision.count(1)/n_neighbours))\n",
    "  \n",
    "  return fea_label, id_pred, label_pred, data_frame_1, count, train_label, train_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_metrics(label_gt,id_pred):\n",
    "  TP,FP,FN,TN = 0,0,0,0\n",
    "\n",
    "  for tp in id_pred[1]:   # TP --> When correctly classified covid\n",
    "    if tp in label_gt[1]:\n",
    "      TP +=1\n",
    "\n",
    "  for tn in id_pred[0]:  # TN --> When correctly classified healthy (non-covid)\n",
    "    if tn in label_gt[0]:\n",
    "      TN +=1\n",
    "\n",
    "  for fp in id_pred[1]: # FP --> When incorrectly classified healthy (Classified healthy as covid)\n",
    "    if fp in label_gt[0]:\n",
    "      FP +=1\n",
    "\n",
    "  for fn in id_pred[0]: # FN --> When missed covid classification (Covid cases missed)\n",
    "    if fn in label_gt[1]:\n",
    "      FN +=1\n",
    "\n",
    "  accuracy= (TP+TN)/(TP+TN+FP+FN+1e-5)\n",
    "  specificity = TN/(TN+FP+1e-5)\n",
    "  sensitivity = (TP)/(TP+FN+1e-5)\n",
    "  # f1_score = (2*precision*recall)/(precision + recall)\n",
    "  \n",
    "  print(\"TP: \", TP)\n",
    "  print(\"FP: \", FP)\n",
    "  print(\"FN: \", FN)\n",
    "  print(\"TN: \", TN)\n",
    "\n",
    "  return accuracy, specificity, sensitivity,TP,TN,FP,FN\n",
    "\n",
    "def roc_auc_curve(label_gt,label_pred):\n",
    "  gt_labels= sorted(label_gt[0]+ label_gt[1])  # Contains (id,labels) tuple of binary class \n",
    "  pred_labels = sorted(label_pred[0]+label_pred[1]) # Contains (id,labels) tuple of binary class --> sorted to match each element in gt_labels and pred_labels\n",
    "  y_test = [y for (x,y) in gt_labels]   # Get only the labels\n",
    "  y_scores = [y for (x,y) in pred_labels]\n",
    "  fpr, tpr, threshold = roc_curve(y_test, y_scores)\n",
    "  roc_auc = auc(fpr, tpr)\n",
    "  return roc_auc\n",
    "\n",
    "def cluster_metrics(fea_label,train_label,id_pred):\n",
    "  print(\"Calculating Dunn's index...\")\n",
    "  intra_dist1 = euclidean_distances(fea_label[0]).max()\n",
    "  intra_dist2 = euclidean_distances(fea_label[1]).max()\n",
    "  inter_dist = euclidean_distances(fea_label[0],fea_label[1]).min()\n",
    "\n",
    "  if intra_dist1>intra_dist2:\n",
    "    max_intra_dist= intra_dist1  \n",
    "  else:\n",
    "    max_intra_dist = intra_dist2 \n",
    "\n",
    "  Dunn_index = inter_dist/max_intra_dist\n",
    "\n",
    "  print(\"Calculating Davies Bouldin index...\")\n",
    "\n",
    "  # Davies Bouldin and Silhouette score from sklearn library.\n",
    "  class_0 =np.concatenate((np.zeros(shape=(len(train_label[0])),dtype=int),np.zeros(shape=(len(id_pred[0])),dtype=int)))\n",
    "  class_1 = np.concatenate((np.ones(shape=(len(train_label[1])),dtype=int),np.ones(shape=(len(id_pred[1])),dtype=int)))\n",
    "  class_all = np.concatenate((class_0,class_1))\n",
    "  feature_all = np.concatenate((fea_label[0],fea_label[1]))\n",
    "\n",
    "  davies_bouldin_index = davies_bouldin_score(feature_all,class_all)\n",
    "  silhouette_index = silhouette_score(feature_all,class_all)\n",
    "\n",
    "  print(\"davies: \", davies_bouldin_index)\n",
    "  print(\"silhouette_sklearn: \", silhouette_index)\n",
    "  \n",
    "  return Dunn_index,davies_bouldin_index, silhouette_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labeled_size = [240,640,840,1250]\n",
    "labeled_size = [240,1250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def data_loader(dataset,n): # Function to return three sets of labeled dataset for experiment\n",
    "#   labeled_data, unlabeled_data = [], [] \n",
    "\n",
    "#   l_data = dataset[:n]    # First dataset // labeled\n",
    "#   ul_data = dataset[n:]   # First dataset // unlabeled\n",
    "#   labeled_data.append(l_data)\n",
    "#   unlabeled_data.append(ul_data)\n",
    "\n",
    "#   l_data = dataset[901:901+n]    # second dataset // labeled\n",
    "#   ul_data = dataset[:901]+dataset[901+n:]\n",
    "#   labeled_data.append(l_data)\n",
    "#   unlabeled_data.append(ul_data)\n",
    "\n",
    "#   l_data = dataset[1802:1802+n]     # Third dataset // labeled\n",
    "#   ul_data = dataset[:1802]+dataset[1802+n:]\n",
    "#   labeled_data.append(l_data)\n",
    "#   unlabeled_data.append(ul_data)\n",
    "#   return labeled_data, unlabeled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def data_loader(dataset,n): # Function to return three sets of labeled dataset for experiment\n",
    "#   labeled_data, unlabeled_data = [], [] \n",
    "\n",
    "#   l_data = dataset[:n]    # First dataset // labeled\n",
    "#   ul_data = dataset[n:]   # First dataset // unlabeled\n",
    "#   labeled_data.append(l_data)\n",
    "#   unlabeled_data.append(ul_data)\n",
    "\n",
    "#   l_data = dataset[437:437+n]    # second dataset // labeled\n",
    "#   ul_data = dataset[:437]+dataset[437+n:]\n",
    "#   labeled_data.append(l_data)\n",
    "#   unlabeled_data.append(ul_data)\n",
    "\n",
    "#   l_data = dataset[874:874+n]     # Third dataset // labeled\n",
    "#   ul_data = dataset[:874]+dataset[874+n:]\n",
    "#   labeled_data.append(l_data)\n",
    "#   unlabeled_data.append(ul_data)\n",
    "#   return labeled_data, unlabeled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(dataset,n): # Function to return three sets of labeled dataset for experiment\n",
    "  labeled_data, unlabeled_data = [], [] \n",
    "\n",
    "  l_data = dataset[:n]    # First dataset // labeled\n",
    "  ul_data = dataset[n:]   # First dataset // unlabeled\n",
    "  labeled_data.append(l_data)\n",
    "  unlabeled_data.append(ul_data)\n",
    "\n",
    "  l_data = dataset[200:200+n]    # second dataset // labeled\n",
    "  ul_data = dataset[:200]+dataset[200+n:]\n",
    "  labeled_data.append(l_data)\n",
    "  unlabeled_data.append(ul_data)\n",
    "\n",
    "  l_data = dataset[300:300+n]     # Third dataset // labeled\n",
    "  ul_data = dataset[:300]+dataset[300+n:]\n",
    "  labeled_data.append(l_data)\n",
    "  unlabeled_data.append(ul_data)\n",
    "  return labeled_data, unlabeled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_separation(dataset,label):\n",
    "    add_data, tup_id_label= [], []\n",
    "    i=0\n",
    "    while len(add_data)!=20:\n",
    "        # print(i)\n",
    "        if dataset[i][\"label\"]==label:\n",
    "            add_data.append(dataset[i]['feature'])\n",
    "            tup_id_label.append((dataset[i][\"id\"],dataset[i]['label']))\n",
    "            del dataset[i]\n",
    "        i+=1\n",
    "    return add_data, tup_id_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labeled data length 3\n",
      "Unlabeled data length 3\n",
      "Feature length neg: 20\n",
      "Feature length pos: 20 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1072/1072 [00:38<00:00, 27.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP:  177\n",
      "FP:  75\n",
      "FN:  369\n",
      "TN:  451\n",
      "Labeled image: 240 \t Dataset: d_0 \t K: 5 \t Accuracy: 0.5858208900576409 \t Specificity: 0.857414432368547 \t Sensitivity: 0.32417581823853814 \t AUC: 0.6491124528196772 \t Corrected count: 74\n",
      "Feature length neg: 20\n",
      "Feature length pos: 20 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1072/1072 [00:38<00:00, 27.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP:  118\n",
      "FP:  31\n",
      "FN:  428\n",
      "TN:  495\n",
      "Labeled image: 240 \t Dataset: d_0 \t K: 9 \t Accuracy: 0.5718283528747355 \t Specificity: 0.9410646208923076 \t Sensitivity: 0.21611721215902543 \t AUC: 0.6479390381481636 \t Corrected count: 68\n",
      "Feature length neg: 20\n",
      "Feature length pos: 20 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1072/1072 [00:42<00:00, 25.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP:  59\n",
      "FP:  10\n",
      "FN:  487\n",
      "TN:  516\n",
      "Labeled image: 240 \t Dataset: d_0 \t K: 15 \t Accuracy: 0.5363805920113751 \t Specificity: 0.9809885745059207 \t Sensitivity: 0.10805860607951272 \t AUC: 0.6446468613768994 \t Corrected count: 78\n",
      "Feature length neg: 20\n",
      "Feature length pos: 20 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1072/1072 [00:42<00:00, 25.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP:  44\n",
      "FP:  8\n",
      "FN:  502\n",
      "TN:  518\n",
      "Labeled image: 240 \t Dataset: d_0 \t K: 19 \t Accuracy: 0.5242537264528571 \t Specificity: 0.9847908558024553 \t Sensitivity: 0.08058607911014508 \t AUC: 0.6386109138010279 \t Corrected count: 85\n",
      "Feature length neg: 20\n",
      "Feature length pos: 20 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1072/1072 [00:43<00:00, 24.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP:  25\n",
      "FP:  4\n",
      "FN:  521\n",
      "TN:  522\n",
      "Labeled image: 240 \t Dataset: d_0 \t K: 25 \t Accuracy: 0.5102611892699516 \t Specificity: 0.9923954183955244 \t Sensitivity: 0.04578754494894607 \t AUC: 0.6354771654201312 \t Corrected count: 83\n",
      "Feature length neg: 20\n",
      "Feature length pos: 20 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1072/1072 [00:42<00:00, 25.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP:  13\n",
      "FP:  0\n",
      "FN:  533\n",
      "TN:  526\n",
      "Labeled image: 240 \t Dataset: d_0 \t K: 31 \t Accuracy: 0.502798502772402 \t Specificity: 0.9999999809885936 \t Sensitivity: 0.023809523373451952 \t AUC: 0.634089611275923 \t Corrected count: 91\n",
      "Feature length neg: 20\n",
      "Feature length pos: 20 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1072/1072 [00:41<00:00, 25.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP:  3\n",
      "FP:  0\n",
      "FN:  543\n",
      "TN:  526\n",
      "Labeled image: 240 \t Dataset: d_0 \t K: 39 \t Accuracy: 0.49347014465046507 \t Specificity: 0.9999999809885936 \t Sensitivity: 0.005494505393873528 \t AUC: 0.6327420994721376 \t Corrected count: 85\n",
      "labeled data length 3\n",
      "Unlabeled data length 3\n",
      "Feature length neg: 20\n",
      "Feature length pos: 20 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62/62 [00:03<00:00, 17.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP:  18\n",
      "FP:  8\n",
      "FN:  17\n",
      "TN:  19\n",
      "Labeled image: 1250 \t Dataset: d_0 \t K: 5 \t Accuracy: 0.5967740972945004 \t Specificity: 0.7037034430727989 \t Sensitivity: 0.5142855673469807 \t AUC: 0.6248677248677248 \t Corrected count: 414\n",
      "Feature length neg: 20\n",
      "Feature length pos: 20 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62/62 [00:03<00:00, 18.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP:  19\n",
      "FP:  10\n",
      "FN:  16\n",
      "TN:  17\n",
      "Labeled image: 1250 \t Dataset: d_0 \t K: 9 \t Accuracy: 0.5806450676378923 \t Specificity: 0.6296293964335569 \t Sensitivity: 0.5428569877551463 \t AUC: 0.6280423280423281 \t Corrected count: 415\n",
      "Feature length neg: 20\n",
      "Feature length pos: 20 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62/62 [00:03<00:00, 18.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP:  17\n",
      "FP:  9\n",
      "FN:  18\n",
      "TN:  18\n",
      "Labeled image: 1250 \t Dataset: d_0 \t K: 15 \t Accuracy: 0.5645160379812841 \t Specificity: 0.6666664197531779 \t Sensitivity: 0.4857141469388151 \t AUC: 0.6343915343915344 \t Corrected count: 421\n",
      "Feature length neg: 20\n",
      "Feature length pos: 20 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62/62 [00:03<00:00, 19.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP:  21\n",
      "FP:  10\n",
      "FN:  14\n",
      "TN:  17\n",
      "Labeled image: 1250 \t Dataset: d_0 \t K: 19 \t Accuracy: 0.6129031269511085 \t Specificity: 0.6296293964335569 \t Sensitivity: 0.5999998285714775 \t AUC: 0.6513227513227514 \t Corrected count: 423\n",
      "Feature length neg: 20\n",
      "Feature length pos: 20 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62/62 [00:03<00:00, 18.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP:  20\n",
      "FP:  8\n",
      "FN:  15\n",
      "TN:  19\n",
      "Labeled image: 1250 \t Dataset: d_0 \t K: 25 \t Accuracy: 0.6290321566077166 \t Specificity: 0.7037034430727989 \t Sensitivity: 0.5714284081633119 \t AUC: 0.671957671957672 \t Corrected count: 419\n",
      "Feature length neg: 20\n",
      "Feature length pos: 20 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62/62 [00:03<00:00, 18.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP:  20\n",
      "FP:  5\n",
      "FN:  15\n",
      "TN:  22\n",
      "Labeled image: 1250 \t Dataset: d_0 \t K: 31 \t Accuracy: 0.677419245577541 \t Specificity: 0.8148145130316619 \t Sensitivity: 0.5714284081633119 \t AUC: 0.6671957671957671 \t Corrected count: 429\n",
      "Feature length neg: 20\n",
      "Feature length pos: 20 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62/62 [00:03<00:00, 18.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP:  20\n",
      "FP:  5\n",
      "FN:  15\n",
      "TN:  22\n",
      "Labeled image: 1250 \t Dataset: d_0 \t K: 39 \t Accuracy: 0.677419245577541 \t Specificity: 0.8148145130316619 \t Sensitivity: 0.5714284081633119 \t AUC: 0.6518518518518519 \t Corrected count: 415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_neighbors=[5,9,15,19,25,31,39]\n",
    "data_frame = {\"Labeled data\": [],\n",
    "              \"Dataset\": [],\n",
    "              \"Accuracy\": [],\n",
    "              \"Specificity\": [],\n",
    "              \"Sensitivity\": [],\n",
    "              \"AUC\":[],\n",
    "              # \"Dunn index\": [],\n",
    "              # \"Davies Bouldin\": [],\n",
    "              # \"Silhouette index\":[],\n",
    "              \"K\": [],\n",
    "              \"TP\":[],\n",
    "              \"TN\":[],\n",
    "              \"FP\":[],\n",
    "              \"FN\":[],\n",
    "              \"pos_labeled_img\":[],\n",
    "              \"neg_labeled_img\":[],\n",
    "              \"corrected_count\":[]\n",
    "    \n",
    "}\n",
    "\n",
    "for size in labeled_size:\n",
    "  labeled_data, unlabeled_data = data_loader(dataset, size)\n",
    "  print(f\"labeled data length {len(labeled_data)}\")\n",
    "  print(f\"Unlabeled data length {len(unlabeled_data)}\")\n",
    "  select=0         # To select the dataset out of three sets ==> three sets: [d11, d12, d13] ==> eg: [200,200,200]\n",
    "  \n",
    "  while(select < 1):\n",
    "    data_frame_1 = {  \"Image name\": [],\n",
    "                  \"Mistake index\": [],\n",
    "                  \"Mistake ID\": [],\n",
    "                  \"Original label\": [],\n",
    "                  \"Predicted label\": []\n",
    "                  \n",
    "}\n",
    "    for neighbor in n_neighbors:\n",
    "      labeled_data, unlabeled_data = data_loader(dataset, size)\n",
    "      pos_img,neg_img=0,0\n",
    "      # mis_predict_id = {0: [],    \n",
    "      # 1 :[]}\n",
    "\n",
    "      label_gt = {0: [],    \n",
    "      1 :[]}    \n",
    "                          # Collect the ground truth (label) of all the predicting images\n",
    "      train_label = {0: [],    \n",
    "      1 :[]}    \n",
    "\n",
    "      label_pred = {0: [],\n",
    "      1 :[]}               # Collect the predicted label for all the images\n",
    "\n",
    "      id_gt = {0: [], \n",
    "          1: [] }         # Collect the ground truth (id) of all the predicting images\n",
    "\n",
    "      id_pred = {0: [],\n",
    "            1: []}        # Collect the predicted id for all the images \n",
    "\n",
    "      fea_label = {0: [],\n",
    "            1: []}\n",
    "\n",
    "      train_id ={0: [],\n",
    "          1:[]}\n",
    "      \n",
    "      fpositive, ptup_id_label = data_separation(labeled_data[select],1)\n",
    "      fnegative, ntup_id_label = data_separation(labeled_data[select],0)\n",
    "      # training_data, supervised_data = labeled_data[select][:20], labeled_data[select][20:]\n",
    "      \n",
    "      # for data in labeled_data[select]:\n",
    "      fea_label={ 0: fnegative,\n",
    "                    1: fpositive\n",
    "        }\n",
    "      \n",
    "      train_label = {0: ntup_id_label, \n",
    "                    1: ptup_id_label}\n",
    "      \n",
    "      # for data in training_data:\n",
    "\n",
    "      #   if data[\"label\"] == 1:\n",
    "      #     fea_label[1].append(data['feature'])\n",
    "      #     train_id[1].append(data['id'])\n",
    "      #     train_label[1].append((data['id'],data['label']))\n",
    "      #     pos_img +=1\n",
    "\n",
    "      #   else:\n",
    "      #     fea_label[0].append(data['feature'])\n",
    "      #     train_id[0].append(data['id'])\n",
    "      #     train_label[0].append((data['id'],data['label']))\n",
    "      #     neg_img +=1\n",
    "      \n",
    "      print(f\"Feature length neg: {len(fea_label[0])}\")\n",
    "      print(f\"Feature length pos: {len(fea_label[1])} \")  \n",
    "\n",
    "      # supervised_data= True\n",
    "      count,ind_data=0,40\n",
    "      for data in labeled_data[select]:\n",
    "        fea_label, id_pred, label_pred, data_frame_1, count, train_label, train_id=distance1(data,fea_label,1,id_pred,label_pred,neighbor, count, train_label, train_id, ind_data, data_frame_1 ,supervised_data=True)\n",
    "        \n",
    "        ind_data +=1\n",
    "      data_f_1 = pd.DataFrame.from_dict(data_frame_1)\n",
    "      # data_f_1.to_csv(f\"../cough_sound_csv/densenet169_cosine_mistake_{size}_{select}.csv\",index=False)\n",
    "        \n",
    "      # # supervised_data = False\n",
    "      for data in tqdm(unlabeled_data[select]):\n",
    "        if data[\"label\"]==1:\n",
    "          id_gt[1].append(data['id'])\n",
    "          label_gt[1].append((data['id'],data['label']))\n",
    "        \n",
    "        else:\n",
    "          id_gt[0].append(data['id'])\n",
    "          label_gt[0].append((data['id'],data['label']))\n",
    "\n",
    "        fea_label,id_pred,label_pred,_,_,_,_ = distance1(data,fea_label,1,id_pred,label_pred,neighbor,count=None,train_label=None, train_id=None, ind_data=None, data_frame_1=None, supervised_data=False)  \n",
    "      accuracy, specificity, sensitivity,TP,TN,FP,FN= classification_metrics(id_gt,id_pred)\n",
    "      # dunn_index, davies_bouldin_index, silhouette_index = cluster_metrics(fea_label,train_label,id_pred)\n",
    "      cl_auc = roc_auc_curve(label_gt,label_pred)\n",
    "      data_frame[\"Labeled data\"].append(size)\n",
    "      data_frame[\"Dataset\"].append(f\"d_{select}\")\n",
    "      data_frame[\"Accuracy\"].append(accuracy)\n",
    "      data_frame[\"Specificity\"].append(specificity)\n",
    "      data_frame[\"Sensitivity\"].append(sensitivity)\n",
    "      data_frame[\"AUC\"].append(cl_auc)\n",
    "      # data_frame[\"Dunn index\"].append(dunn_index)\n",
    "      # data_frame[\"Davies Bouldin\"].append(davies_bouldin_index)\n",
    "      # data_frame[\"Silhouette index\"].append(silhouette_index)\n",
    "      data_frame[\"K\"].append(neighbor)\n",
    "      data_frame[\"TP\"].append(TP)\n",
    "      data_frame[\"TN\"].append(TN)\n",
    "      data_frame[\"FP\"].append(FP)\n",
    "      data_frame[\"FN\"].append(FN)\n",
    "      data_frame[\"pos_labeled_img\"].append(pos_img)\n",
    "      data_frame[\"neg_labeled_img\"].append(neg_img)\n",
    "      data_frame[\"corrected_count\"].append(count)\n",
    "\n",
    "      print(f\"Labeled image: {size} \\t Dataset: d_{select} \\t K: {neighbor} \\t Accuracy: {accuracy} \\t Specificity: {specificity} \\t Sensitivity: {sensitivity} \\t AUC: {cl_auc} \\t Corrected count: {count}\")\n",
    "      # select +=1 \n",
    "      data_f=pd.DataFrame.from_dict(data_frame)\n",
    "      data_f.to_csv(f\"../knn_plot_csv/cs_vgg16_euclidean_dist_200_1210.csv\",index=False)\n",
    "    select +=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_model= \"VGG16\"\n",
    "s_distance=\"Euclidean_dist\"\n",
    "len(labeled_data[select])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_f=pd.DataFrame.from_dict(data_frame)\n",
    "data_f.to_csv(f\"./csv_results_x-ray_counts/{s_model}_{s_distance}_dist_1100_1300.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
